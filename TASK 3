!pip install tensorflow nltk flask-ngrok flask pyngrok flask_cors
#Installing necessary libraries
import numpy as np
import random
import json
import pickle
import nltk
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import SGD

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()
#intents.json 
import json

intents = {
  "intents": [
    {
      "tag": "greeting",
      "patterns": ["Hi", "Hello", "Hey"],
      "responses": ["Hello!", "Hi there!", "Greetings!"]
    },
    {
      "tag": "order_status",
      "patterns": ["Where is my order?", "Track my package", "Order status"],
      "responses": ["Please provide your order ID.", "Tracking your order..."]
    },
    {
      "tag": "refund_policy",
      "patterns": ["What is your refund policy?", "Can I get a refund?"],
      "responses": ["You can request a refund within 30 days."]
    },
    {
      "tag": "goodbye",
      "patterns": ["Bye", "See you later", "Goodbye"],
      "responses": ["Goodbye!", "Have a great day!"]
    }
  ]
}

# Save to intents.json
with open('intents.json', 'w') as f:
    json.dump(intents, f, indent=2)
# Load the intents file
import json
import nltk
import numpy as np
import random
import pickle
from nltk.stem import WordNetLemmatizer
import os

# 1. FIRST FIX THE NLTK DATA ISSUE
# --------------------------------
# Clean existing NLTK data
!rm -rf /root/nltk_data

# Set correct path and download with force
os.makedirs('/root/nltk_data', exist_ok=True)
nltk.data.path.clear()
nltk.data.path.append('/root/nltk_data')

# Download required resources with force
nltk.download('punkt', force=True)
nltk.download('wordnet', force=True)
nltk.download('omw-1.4', force=True)

# Manual download of punkt_tab as fallback
!wget -q https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt.zip -P /root/nltk_data/tokenizers/
!unzip -o -q /root/nltk_data/tokenizers/punkt.zip -d /root/nltk_data/tokenizers/

# 2. LOAD AND PREPROCESS DATA WITH ERROR HANDLING
# ----------------------------------------------
# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Load intents
try:
    with open('intents.json') as file:
        data = json.load(file)
except Exception as e:
    print(f"Error loading intents.json: {str(e)}")
    raise

words = []
classes = []
documents = []
ignore_letters = ['?', '!', '.', ',']

# Robust tokenization with fallback
def safe_tokenize(text):
    try:
        return nltk.word_tokenize(text)
    except:
        return text.split()  # Simple fallback

# Process patterns
for intent in data['intents']:
    for pattern in intent['patterns']:
        word_list = safe_tokenize(pattern)
        words.extend(word_list)
        documents.append((word_list, intent['tag']))
        if intent['tag'] not in classes:
            classes.append(intent['tag'])
if predicted_confidence < 0.6:
    response = "I'm sorry, I didn't quite understand that. Could you rephrase?"
else:
    response = choose_response(predicted_intent)


# Lemmatization and cleanup
words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_letters]
words = sorted(set(words))
classes = sorted(set(classes))

# Save vocabulary
with open('words.pkl', 'wb') as f:
    pickle.dump(words, f)
with open('classes.pkl', 'wb') as f:
    pickle.dump(classes, f)

# 3. FIX THE TRAINING DATA PREPARATION
# ------------------------------------
training = []
output_empty = [0] * len(classes)

for document in documents:
    bag = []
    word_patterns = document[0]
    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]

    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)

    output_row = list(output_empty)
    output_row[classes.index(document[1])] = 1
    training.append([bag, output_row])

# Fix for IndexError - ensure training is 2D before converting to numpy array
if len(training) > 0:
    training = np.array(training, dtype=object)
    train_x = list(training[:, 0])
    train_y = list(training[:, 1])
else:
    train_x = []
    train_y = []
    print("Warning: No training data was created!")

print("\nPreprocessing completed successfully!")
print(f"Unique words: {len(words)}")
print(f"Classes: {classes}")
print(f"Training samples: {len(train_x)}")

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import SGD
import numpy as np  # Don't forget this if you're using np.array

model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)

model.save('chatbot_model.keras')
print("Model created and saved in modern Keras format")

from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import SGD

# Load the model with custom objects if needed
try:
    model = load_model('chatbot_model.keras')

    # Recompile the model to rebuild metrics
    sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy',
                 optimizer=sgd,
                 metrics=['accuracy'])

    print("Model loaded and recompiled successfully")
except Exception as e:
    print(f"Error loading model: {str(e)}")
    raise

# Load vocabulary
words = pickle.load(open('words.pkl', 'rb'))
classes = pickle.load(open('classes.pkl', 'rb'))

def clean_up_sentence(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words

def bow(sentence, words, show_details=True):
    sentence_words = clean_up_sentence(sentence)
    bag = [0] * len(words)
    for s in sentence_words:
        for i, w in enumerate(words):
            if w == s:
                bag[i] = 1
                if show_details:
                    print(f"found in bag: {w}")
    return np.array(bag)

def predict_class(sentence, model):
    p = bow(sentence, words, show_details=False)
    res = model.predict(np.array([p]))[0]
    ERROR_THRESHOLD = 0.25
    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]

    results.sort(key=lambda x: x[1], reverse=True)
    return_list = []
    for r in results:
        return_list.append({"intent": classes[r[0]], "probability": str(r[1])})
    return return_list

def get_response(intents_list, intents_json):
    tag = intents_list[0]['intent']
    list_of_intents = intents_json['intents']
    for i in list_of_intents:
        if i['tag'] == tag:
            result = random.choice(i['responses'])
            break
    return result

def chatbot_response(msg):
    ints = predict_class(msg, model)
    res = get_response(ints, intents)
    return res

from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok
import nltk
import json
import random
import os
from nltk.stem import WordNetLemmatizer

# Download NLTK data
nltk.download('punkt')
nltk.download('wordnet')
from google.colab import files
uploaded = files.upload()


# Load intents
# Load intents again
with open('intents.json') as file:
    intents = json.load(file)

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Preprocess user input
def preprocess(sentence):
    tokens = nltk.word_tokenize(sentence.lower())
    return [lemmatizer.lemmatize(word) for word in tokens]

# Improved intent classification
def classify_intent(user_input):
    words = preprocess(user_input)
    best_intent = None
    max_overlap = 0

    for intent in intents['intents']:
        for pattern in intent['patterns']:
            pattern_words = preprocess(pattern)
            overlap = len(set(pattern_words) & set(words))
            if overlap > max_overlap:
                max_overlap = overlap
                best_intent = intent

    return best_intent if max_overlap > 0 else None

# Chatbot response generator
def chatbot_response(message):
    intent = classify_intent(message)
    if intent:
        return random.choice(intent['responses'])
    else:
        return "Sorry, I didn't understand that. Could you rephrase?"

# Setup Flask
app = Flask(__name__)
CORS(app)

# Save the HTML UI to templates/index.html
html_template = """<!DOCTYPE html>
<html>
<head>
    <title>Customer Support Chatbot</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background-color: #f5f5f5;
        }
        .chat-container {
            width: 400px;
            height: 600px;
            border: 1px solid #ddd;
            border-radius: 8px;
            overflow: hidden;
            display: flex;
            flex-direction: column;
            background-color: white;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        .chat-header {
            background-color: #4a6fa5;
            color: white;
            padding: 15px;
            text-align: center;
            font-weight: bold;
        }
        .chat-messages {
            flex: 1;
            padding: 15px;
            overflow-y: auto;
        }
        .message {
            margin-bottom: 15px;
            max-width: 80%;
            padding: 10px 15px;
            border-radius: 18px;
            line-height: 1.4;
        }
        .user-message {
            background-color: #e3f2fd;
            margin-left: auto;
            border-bottom-right-radius: 5px;
        }
        .bot-message {
            background-color: #f1f1f1;
            margin-right: auto;
            border-bottom-left-radius: 5px;
        }
        .chat-input {
            display: flex;
            padding: 10px;
            border-top: 1px solid #ddd;
            background-color: #f9f9f9;
        }
        #userInput {
            flex: 1;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 20px;
            outline: none;
        }
        #sendButton {
            margin-left: 10px;
            padding: 10px 20px;
            background-color: #4a6fa5;
            color: white;
            border: none;
            border-radius: 20px;
            cursor: pointer;
        }
        #sendButton:hover {
            background-color: #3a5a80;
        }
    </style>
</head>
<body>
    <div class="chat-container">
        <div class="chat-header">Customer Support Chatbot</div>
        <div class="chat-messages" id="chatBox"></div>
        <div class="chat-input">
            <input type="text" id="userInput" placeholder="Type your message here..." />
            <button id="sendButton">Send</button>
        </div>
    </div>

    <script>
        const chatBox = document.getElementById('chatBox');
        const userInput = document.getElementById('userInput');
        const sendButton = document.getElementById('sendButton');

        function addMessage(message, isUser) {
            const messageDiv = document.createElement('div');
            messageDiv.classList.add('message');
            messageDiv.classList.add(isUser ? 'user-message' : 'bot-message');
            messageDiv.textContent = message;
            chatBox.appendChild(messageDiv);
            chatBox.scrollTop = chatBox.scrollHeight;
        }

        function sendMessage() {
            const message = userInput.value.trim();
            if (message) {
                addMessage(message, true);
                userInput.value = '';

                fetch('/get', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({ msg: message }),
                })
                .then(response => response.json())
                .then(data => {
                    addMessage(data.response, false);
                })
                .catch(error => {
                    console.error('Error:', error);
                    addMessage("Sorry, I'm having trouble responding right now.", false);
                });
            }
        }

        sendButton.addEventListener('click', sendMessage);
        userInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });

        // Initial greeting
        setTimeout(() => {
            addMessage("Hello! I'm your customer support assistant. How can I help you today?", false);
        }, 500);
    </script>
</body>
</html>
"""

# Save HTML template
os.makedirs('templates', exist_ok=True)
with open('templates/index.html', 'w') as f:
    f.write(html_template)

# Define routes
@app.route("/")
def home():
    return render_template("index.html")

@app.route("/get", methods=["POST"])
def get_bot_response():
    user_text = request.json.get("msg")
    return jsonify({"response": chatbot_response(user_text)})

# Expose app using ngrok
ngrok.set_auth_token("2xZNhIqrGyJE3NznU1byHWpUBUp_6AyyKiSX3f8w2GNq7PATE")  # Replace with your token
port = 5000
public_url = ngrok.connect(port)
print("ðŸ”— Public URL:", public_url)
print("ðŸš€ App is running!")

# Run the app
app.run(port=port)

